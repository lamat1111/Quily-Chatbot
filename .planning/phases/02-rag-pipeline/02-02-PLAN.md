---
phase: 02-rag-pipeline
plan: 02
type: execute
wave: 2
depends_on: ["02-01"]
files_modified:
  - src/lib/rag/prompt.ts
  - app/api/chat/route.ts
autonomous: true
user_setup:
  - service: cohere
    why: "Optional reranking for improved precision"
    env_vars:
      - name: COHERE_API_KEY
        source: "Cohere Dashboard -> API Keys (https://dashboard.cohere.com/api-keys)"
    note: "Optional - reranking is skipped if not provided"

must_haves:
  truths:
    - "POST /api/chat accepts messages, apiKey, model and streams response"
    - "Retrieved chunks are formatted with numbered citations in system prompt"
    - "Response includes source metadata before LLM tokens stream"
    - "LLM is instructed to cite sources using [1], [2], etc."
  artifacts:
    - path: "src/lib/rag/prompt.ts"
      provides: "Context formatting and system prompt builder"
      exports: ["buildContextBlock", "buildSystemPrompt", "formatSourcesForClient"]
    - path: "app/api/chat/route.ts"
      provides: "Streaming chat endpoint"
      exports: ["POST"]
  key_links:
    - from: "app/api/chat/route.ts"
      to: "src/lib/rag/retriever.ts"
      via: "retrieveWithReranking import"
      pattern: "import.*retrieveWithReranking.*from"
    - from: "app/api/chat/route.ts"
      to: "src/lib/rag/prompt.ts"
      via: "prompt builder imports"
      pattern: "import.*buildSystemPrompt.*from"
    - from: "app/api/chat/route.ts"
      to: "streamText"
      via: "Vercel AI SDK streaming"
      pattern: "streamText\\("
---

<objective>
Build the prompt assembly layer and streaming chat API endpoint.

Purpose: Complete the RAG pipeline - take retrieved chunks, format into prompt with citations, stream LLM response with source metadata.
Output: Working /api/chat endpoint that accepts user query and streams AI response with source references.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/02-rag-pipeline/02-RESEARCH.md
@.planning/phases/02-rag-pipeline/02-01-SUMMARY.md

# Dependencies from Plan 01
@src/lib/rag/types.ts
@src/lib/rag/retriever.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create prompt builder module</name>
  <files>src/lib/rag/prompt.ts</files>
  <action>
Create `src/lib/rag/prompt.ts` with three exports:

1. `buildContextBlock(chunks: RetrievedChunk[]): string`
   - If no chunks, return "No relevant documentation found."
   - For each chunk, format as:
     ```
     [N] Source: {source_file} > {heading_path}
     ---
     {content}
     ```
   - Join with double newlines
   - Use citationIndex for [N]

2. `buildSystemPrompt(context: string, chunkCount: number): string`
   - Return a prompt that:
     - Identifies as Quilibrium protocol assistant
     - Instructs to answer ONLY from provided context
     - Instructs to use citations [1] through [N] (with actual N value)
     - Instructs to say "I don't have specific information about that" if context doesn't help
     - Instructs to use markdown for code blocks
     - Warns never to invent citation numbers beyond provided
   - Embeds the context block

3. `formatSourcesForClient(chunks: RetrievedChunk[]): SourceReference[]`
   - Maps each chunk to { index, file, heading, url }
   - For url: if source_file starts with 'docs/', construct URL like:
     `https://docs.quilibrium.com/${path without docs/ and .md}`
   - Otherwise url is null

Import RetrievedChunk and SourceReference from './types'.
  </action>
  <verify>
`cat src/lib/rag/prompt.ts` shows all three exports with:
- Citation [N] formatting
- System prompt with citation instructions
- URL generation logic

`npx tsc --noEmit` passes.
  </verify>
  <done>Prompt module exports buildContextBlock, buildSystemPrompt, formatSourcesForClient</done>
</task>

<task type="auto">
  <name>Task 2: Create streaming chat API route</name>
  <files>app/api/chat/route.ts</files>
  <action>
Create `app/api/chat/route.ts` with POST handler:

1. Import dependencies:
   - `createDataStreamResponse, streamText, convertToModelMessages` from 'ai'
   - `createOpenRouter` from '@openrouter/ai-sdk-provider'
   - `z` from 'zod'
   - `retrieveWithReranking` from '@/lib/rag/retriever'
   - `buildContextBlock, buildSystemPrompt, formatSourcesForClient` from '@/lib/rag/prompt'

2. Define request schema with zod:
   ```typescript
   const requestSchema = z.object({
     messages: z.array(z.object({
       role: z.enum(['user', 'assistant', 'system']),
       content: z.string(),
     })),
     apiKey: z.string().min(1, 'API key required'),
     model: z.string().default('anthropic/claude-3.5-sonnet'),
   });
   ```

3. POST handler:
   - Parse and validate request body with zod
   - Extract last user message (return 400 if none)
   - Call `retrieveWithReranking()` with:
     - query: lastUserMessage.content
     - embeddingApiKey: apiKey from request
     - cohereApiKey: process.env.COHERE_API_KEY (optional)
   - Build context and system prompt
   - Create OpenRouter provider with user's apiKey
   - Use `createDataStreamResponse` pattern:
     - First `dataStream.writeData({ type: 'sources', data: formatSourcesForClient(chunks) })`
     - Then `streamText()` with system prompt and messages
     - `result.mergeIntoDataStream(dataStream)`
   - Return the data stream response

4. Error handling:
   - Zod errors: return 400 with error details
   - Other errors: log and return 500

IMPORTANT: Use `createDataStreamResponse` (not `toUIMessageStreamResponse`) to send sources BEFORE the LLM stream starts. This allows the frontend to display sources while tokens stream.
  </action>
  <verify>
`cat app/api/chat/route.ts` shows:
- POST export
- zod validation
- retrieveWithReranking call
- createDataStreamResponse with sources first
- streamText with system prompt
- Error handling for zod and general errors

Start dev server and test with curl:
```bash
# This will fail without valid API key, but confirms route exists
curl -X POST http://localhost:3000/api/chat \
  -H "Content-Type: application/json" \
  -d '{"messages":[{"role":"user","content":"test"}],"apiKey":"test"}'
```
Should return an error about API key (not 404).
  </verify>
  <done>
Chat route:
- Validates request with zod
- Retrieves context with optional reranking
- Streams sources then LLM response
- Returns proper error codes
  </done>
</task>

<task type="auto">
  <name>Task 3: Configure Next.js and path aliases</name>
  <files>next.config.js, tsconfig.json</files>
  <action>
1. Create `next.config.js` (minimal config):
   ```javascript
   /** @type {import('next').NextConfig} */
   const nextConfig = {
     // Allow streaming responses
     experimental: {
       serverActions: true,
     },
   };

   module.exports = nextConfig;
   ```

2. Update `tsconfig.json` to add path aliases for @/ imports:
   - Add to compilerOptions:
     ```json
     "baseUrl": ".",
     "paths": {
       "@/*": ["./*"]
     }
     ```
   - Ensure "moduleResolution" is "bundler" or "node" (compatible with Next.js)
   - Add "jsx": "preserve" for React support

3. Update `package.json` scripts to add Next.js dev:
   ```json
   "dev": "next dev",
   "build": "next build",
   "start": "next start"
   ```

4. Install Next.js if not present:
   ```bash
   npm install next react react-dom
   npm install --save-dev @types/react @types/react-dom
   ```

Note: The app/ directory with route.ts follows Next.js App Router conventions.
  </action>
  <verify>
`npm run dev` starts without immediate errors (may warn about missing pages, that's OK).
`npx tsc --noEmit` passes.
Path alias `@/lib/rag/retriever` resolves correctly.
  </verify>
  <done>
Next.js configured with:
- Path aliases (@/)
- Dev/build/start scripts
- App Router enabled
  </done>
</task>

</tasks>

<verification>
After all tasks:
1. `npx tsc --noEmit` - no type errors
2. `npm run dev` - Next.js starts
3. File structure:
   - src/lib/rag/prompt.ts
   - app/api/chat/route.ts
   - next.config.js updated
4. Manual test: POST to /api/chat returns error about API key (not 404)
</verification>

<success_criteria>
- Prompt builder formats chunks with [N] citations
- System prompt instructs LLM to cite sources
- Chat API validates requests with zod
- Chat API streams sources before LLM response
- Chat API uses user's API key for OpenRouter
- Next.js dev server starts without errors
</success_criteria>

<output>
After completion, create `.planning/phases/02-rag-pipeline/02-02-SUMMARY.md`
</output>
