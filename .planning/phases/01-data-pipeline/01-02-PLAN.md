---
phase: 01-data-pipeline
plan: 02
type: execute
wave: 2
depends_on: ["01-01"]
files_modified:
  - scripts/ingest/loader.ts
  - scripts/ingest/chunker.ts
autonomous: true

must_haves:
  truths:
    - "Loader finds all markdown files in a directory"
    - "Loader extracts frontmatter from markdown files"
    - "Chunker splits text at semantic boundaries (headings, paragraphs)"
    - "Chunks are 500-1000 tokens with 100 token overlap"
    - "Each chunk includes heading path metadata"
  artifacts:
    - path: "scripts/ingest/loader.ts"
      provides: "Markdown file loading with frontmatter parsing"
      exports: ["loadDocuments"]
    - path: "scripts/ingest/chunker.ts"
      provides: "Token-based text chunking with heading context"
      exports: ["chunkDocuments"]
  key_links:
    - from: "scripts/ingest/loader.ts"
      to: "scripts/ingest/types.ts"
      via: "import LoadedDocument"
      pattern: "import.*LoadedDocument.*from"
    - from: "scripts/ingest/chunker.ts"
      to: "@langchain/textsplitters"
      via: "RecursiveCharacterTextSplitter"
      pattern: "RecursiveCharacterTextSplitter"
    - from: "scripts/ingest/chunker.ts"
      to: "gpt-tokenizer"
      via: "token counting"
      pattern: "encode.*from.*gpt-tokenizer"
---

<objective>
Implement markdown loading and semantic chunking for the ingestion pipeline.

Purpose: These modules transform raw markdown files into properly-sized chunks with metadata, ready for embedding.
Output: loader.ts that finds and loads markdown files, chunker.ts that splits them with heading context preservation.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/01-data-pipeline/01-RESEARCH.md
@scripts/ingest/types.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create markdown document loader</name>
  <files>scripts/ingest/loader.ts</files>
  <action>
Create the loader module that finds and loads markdown files:

```typescript
import { glob } from 'glob';
import { readFile } from 'fs/promises';
import { join, relative } from 'path';
import type { LoadedDocument } from './types.js';

/**
 * Parse frontmatter from markdown content
 * Returns content without frontmatter and parsed frontmatter object
 */
function parseFrontmatter(content: string): {
  content: string;
  frontmatter: Record<string, unknown> | undefined;
} {
  const frontmatterRegex = /^---\n([\s\S]*?)\n---\n/;
  const match = content.match(frontmatterRegex);

  if (!match) {
    return { content, frontmatter: undefined };
  }

  // Simple YAML-like parsing (key: value pairs)
  const frontmatter: Record<string, unknown> = {};
  const lines = match[1].split('\n');
  for (const line of lines) {
    const colonIndex = line.indexOf(':');
    if (colonIndex > 0) {
      const key = line.slice(0, colonIndex).trim();
      const value = line.slice(colonIndex + 1).trim();
      // Remove quotes if present
      frontmatter[key] = value.replace(/^["']|["']$/g, '');
    }
  }

  return {
    content: content.slice(match[0].length),
    frontmatter,
  };
}

/**
 * Load all markdown files from a directory
 * @param docsPath - Path to documentation directory
 * @returns Array of loaded documents with path and content
 */
export async function loadDocuments(docsPath: string): Promise<LoadedDocument[]> {
  // Find all markdown files recursively
  const pattern = join(docsPath, '**/*.md');
  const files = await glob(pattern, { nodir: true });

  if (files.length === 0) {
    throw new Error(`No markdown files found in ${docsPath}`);
  }

  const documents: LoadedDocument[] = [];

  for (const filePath of files) {
    const rawContent = await readFile(filePath, 'utf-8');
    const { content, frontmatter } = parseFrontmatter(rawContent);
    const relativePath = relative(docsPath, filePath);

    documents.push({
      path: relativePath,
      content,
      frontmatter,
    });
  }

  return documents;
}
```

Key design decisions:
- Uses glob for cross-platform file finding
- Parses frontmatter separately (Docusaurus-compatible)
- Returns relative paths for cleaner source_file metadata
- Throws if no files found (fail fast)
  </action>
  <verify>
Run `npx tsc --noEmit` (should compile without errors)
Check that file exports loadDocuments function
  </verify>
  <done>
loader.ts exports loadDocuments() that finds markdown files and extracts frontmatter
  </done>
</task>

<task type="auto">
  <name>Task 2: Create semantic text chunker with heading context</name>
  <files>scripts/ingest/chunker.ts</files>
  <action>
Create the chunker module that splits documents while preserving heading hierarchy:

```typescript
import { RecursiveCharacterTextSplitter } from '@langchain/textsplitters';
import { encode } from 'gpt-tokenizer';
import { createHash } from 'crypto';
import type { LoadedDocument, ChunkWithContext, ChunkMetadata } from './types.js';

/**
 * Count tokens using cl100k_base tokenizer (matches text-embedding-3-small)
 */
function countTokens(text: string): number {
  return encode(text).length;
}

/**
 * Generate MD5 hash of content for deduplication
 */
function hashContent(content: string): string {
  return createHash('md5').update(content).digest('hex');
}

/**
 * Extract heading hierarchy from markdown content
 * Returns a map of content position -> heading path
 */
function buildHeadingMap(content: string): Map<number, string> {
  const headingMap = new Map<number, string>();
  const headingStack: { level: number; text: string }[] = [];
  const headingRegex = /^(#{1,6})\s+(.+)$/gm;

  let match;
  while ((match = headingRegex.exec(content)) !== null) {
    const level = match[1].length;
    const text = match[2].trim();

    // Pop headings that are same level or higher
    while (headingStack.length > 0 && headingStack[headingStack.length - 1].level >= level) {
      headingStack.pop();
    }

    headingStack.push({ level, text });

    // Build path from current stack
    const path = headingStack.map((h) => h.text).join(' > ');
    headingMap.set(match.index, path);
  }

  return headingMap;
}

/**
 * Find the heading path for a given chunk based on its position
 */
function findHeadingPath(
  chunkStart: number,
  content: string,
  headingMap: Map<number, string>
): string {
  // Find the closest heading before this chunk
  let closestHeading = '';
  let closestPosition = -1;

  for (const [position, path] of headingMap) {
    if (position <= chunkStart && position > closestPosition) {
      closestPosition = position;
      closestHeading = path;
    }
  }

  return closestHeading;
}

/**
 * Chunk documents with semantic boundaries and heading context
 * @param documents - Loaded markdown documents
 * @param version - Version tag for this ingestion run
 * @returns Array of chunks with metadata
 */
export async function chunkDocuments(
  documents: LoadedDocument[],
  version: string
): Promise<ChunkWithContext[]> {
  // Configure splitter for markdown with token-based sizing
  const splitter = RecursiveCharacterTextSplitter.fromLanguage('markdown', {
    chunkSize: 800, // Target 500-1000 tokens
    chunkOverlap: 100, // ~10-15% overlap
    lengthFunction: countTokens,
  });

  const allChunks: ChunkWithContext[] = [];

  for (const doc of documents) {
    // Build heading map for this document
    const headingMap = buildHeadingMap(doc.content);

    // Split the document
    const textChunks = await splitter.splitText(doc.content);

    // Track position in original content for heading lookup
    let searchPosition = 0;

    for (let i = 0; i < textChunks.length; i++) {
      const chunkContent = textChunks[i];

      // Find where this chunk starts in the original content
      const chunkStart = doc.content.indexOf(chunkContent.slice(0, 50), searchPosition);
      if (chunkStart !== -1) {
        searchPosition = chunkStart;
      }

      // Find the heading path for this chunk
      const headingPath = findHeadingPath(chunkStart, doc.content, headingMap);

      const metadata: ChunkMetadata = {
        source_file: doc.path,
        heading_path: headingPath,
        chunk_index: i,
        token_count: countTokens(chunkContent),
        version,
        content_hash: hashContent(chunkContent),
      };

      allChunks.push({
        content: chunkContent,
        metadata,
      });
    }
  }

  return allChunks;
}
```

Key design decisions:
- Uses RecursiveCharacterTextSplitter.fromLanguage('markdown') for proper markdown handling
- Token-based sizing using gpt-tokenizer (matches embedding model tokenizer)
- Tracks heading hierarchy for context (critical for RAG quality)
- MD5 hash enables deduplication on re-ingestion
- 800 token target with 100 overlap = good balance per research
  </action>
  <verify>
Run `npx tsc --noEmit` (should compile without errors)
Check that file exports chunkDocuments function
Check that it imports from @langchain/textsplitters and gpt-tokenizer
  </verify>
  <done>
chunker.ts exports chunkDocuments() that splits markdown with heading context, token-based sizing, and deduplication hashes
  </done>
</task>

</tasks>

<verification>
After completing all tasks:
1. `npx tsc --noEmit` succeeds
2. `ls scripts/ingest/loader.ts scripts/ingest/chunker.ts` shows both files exist
3. Both files have proper TypeScript exports
4. chunker.ts uses RecursiveCharacterTextSplitter (not regex)
5. chunker.ts uses gpt-tokenizer for token counting (not character count)
</verification>

<success_criteria>
- loader.ts can find and load markdown files from a directory
- loader.ts extracts frontmatter properly
- chunker.ts produces chunks of 500-1000 tokens
- chunker.ts preserves heading hierarchy in metadata
- chunker.ts generates content hashes for deduplication
- TypeScript compiles without errors
</success_criteria>

<output>
After completion, create `.planning/phases/01-data-pipeline/01-02-SUMMARY.md`
</output>
