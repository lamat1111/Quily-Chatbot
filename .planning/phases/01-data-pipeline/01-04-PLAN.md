---
phase: 01-data-pipeline
plan: 04
type: execute
wave: 3
depends_on: ["01-02", "01-03"]
files_modified:
  - scripts/ingest/index.ts
autonomous: false

must_haves:
  truths:
    - "CLI script runs full pipeline: load -> chunk -> embed -> upload"
    - "CLI script shows progress during each stage"
    - "CLI script has dry-run mode for testing without upload"
    - "Vector similarity search returns relevant chunks"
  artifacts:
    - path: "scripts/ingest/index.ts"
      provides: "CLI entry point orchestrating full pipeline"
      contains: "program.parse()"
  key_links:
    - from: "scripts/ingest/index.ts"
      to: "scripts/ingest/loader.ts"
      via: "loadDocuments import"
      pattern: "loadDocuments.*from.*loader"
    - from: "scripts/ingest/index.ts"
      to: "scripts/ingest/chunker.ts"
      via: "chunkDocuments import"
      pattern: "chunkDocuments.*from.*chunker"
    - from: "scripts/ingest/index.ts"
      to: "scripts/ingest/embedder.ts"
      via: "generateEmbeddings import"
      pattern: "generateEmbeddings.*from.*embedder"
    - from: "scripts/ingest/index.ts"
      to: "scripts/ingest/uploader.ts"
      via: "uploadChunks import"
      pattern: "uploadChunks.*from.*uploader"
---

<objective>
Create the CLI entry point that orchestrates the full ingestion pipeline and verify end-to-end functionality.

Purpose: This is the user-facing interface to the data pipeline. It ties all modules together and provides feedback during ingestion.
Output: Working CLI script that can ingest documentation into Supabase with progress feedback.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/01-data-pipeline/01-RESEARCH.md
@scripts/ingest/types.ts
@scripts/ingest/loader.ts
@scripts/ingest/chunker.ts
@scripts/ingest/embedder.ts
@scripts/ingest/uploader.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create CLI entry point with commander</name>
  <files>scripts/ingest/index.ts</files>
  <action>
Create the CLI entry point that orchestrates the full pipeline:

```typescript
#!/usr/bin/env node
import 'dotenv/config';
import { Command } from 'commander';
import ora from 'ora';
import chalk from 'chalk';
import { loadDocuments } from './loader.js';
import { chunkDocuments } from './chunker.js';
import { generateEmbeddings } from './embedder.js';
import { uploadChunks, getChunkCount } from './uploader.js';
import type { IngestOptions } from './types.js';

const program = new Command();

program
  .name('ingest')
  .description('Ingest markdown documentation into Supabase vector database')
  .version('1.0.0');

program
  .command('run')
  .description('Run full ingestion pipeline')
  .option('-d, --docs <path>', 'Path to documentation directory', './docs')
  .option('-v, --version <tag>', 'Version tag for chunks', new Date().toISOString().split('T')[0])
  .option('--dry-run', 'Preview without uploading to database', false)
  .action(async (options: IngestOptions) => {
    const spinner = ora();

    // Validate environment
    const { SUPABASE_URL, SUPABASE_SERVICE_KEY, OPENROUTER_API_KEY } = process.env;

    if (!options.dryRun) {
      if (!SUPABASE_URL || !SUPABASE_SERVICE_KEY) {
        console.error(chalk.red('Error: SUPABASE_URL and SUPABASE_SERVICE_KEY must be set'));
        console.error(chalk.gray('Create a .env file with these values from your Supabase dashboard'));
        process.exit(1);
      }
      if (!OPENROUTER_API_KEY) {
        console.error(chalk.red('Error: OPENROUTER_API_KEY must be set'));
        console.error(chalk.gray('Get your API key from https://openrouter.ai/keys'));
        process.exit(1);
      }
    }

    console.log(chalk.blue('\nðŸ“š Quilibrium Docs Ingestion Pipeline\n'));
    console.log(chalk.gray(`  Docs path: ${options.docs}`));
    console.log(chalk.gray(`  Version: ${options.version}`));
    console.log(chalk.gray(`  Dry run: ${options.dryRun}\n`));

    try {
      // Step 1: Load documents
      spinner.start('Loading markdown files...');
      const documents = await loadDocuments(options.docs);
      spinner.succeed(`Loaded ${documents.length} documents`);

      // Step 2: Chunk documents
      spinner.start('Chunking documents...');
      const chunks = await chunkDocuments(documents, options.version);
      const totalTokens = chunks.reduce((sum, c) => sum + c.metadata.token_count, 0);
      spinner.succeed(`Created ${chunks.length} chunks (${totalTokens.toLocaleString()} tokens total)`);

      // In dry-run mode, show sample and exit
      if (options.dryRun) {
        console.log(chalk.yellow('\nðŸ” Dry run - sample chunks:\n'));
        const samples = chunks.slice(0, 3);
        for (const sample of samples) {
          console.log(chalk.gray('â”€'.repeat(60)));
          console.log(chalk.white(`Source: ${sample.metadata.source_file}`));
          console.log(chalk.white(`Heading: ${sample.metadata.heading_path || '(none)'}`));
          console.log(chalk.white(`Tokens: ${sample.metadata.token_count}`));
          console.log(chalk.gray(sample.content.slice(0, 200) + '...'));
        }
        console.log(chalk.gray('â”€'.repeat(60)));
        console.log(chalk.green('\nâœ… Dry run complete. Add real credentials and remove --dry-run to upload.\n'));
        return;
      }

      // Step 3: Generate embeddings
      spinner.start('Generating embeddings...');
      const embeddedChunks = await generateEmbeddings(
        chunks,
        OPENROUTER_API_KEY!,
        (completed, total) => {
          spinner.text = `Generating embeddings... ${completed}/${total}`;
        }
      );
      spinner.succeed(`Generated ${embeddedChunks.length} embeddings`);

      // Step 4: Upload to Supabase
      spinner.start('Uploading to Supabase...');
      const { inserted, errors } = await uploadChunks(
        embeddedChunks,
        SUPABASE_URL!,
        SUPABASE_SERVICE_KEY!,
        (completed, total) => {
          spinner.text = `Uploading to Supabase... ${completed}/${total}`;
        }
      );

      if (errors.length > 0) {
        spinner.warn(`Uploaded ${inserted} chunks with ${errors.length} errors`);
        for (const err of errors) {
          console.error(chalk.red(`  - ${err}`));
        }
      } else {
        spinner.succeed(`Uploaded ${inserted} chunks`);
      }

      // Step 5: Verify
      spinner.start('Verifying...');
      const totalCount = await getChunkCount(SUPABASE_URL!, SUPABASE_SERVICE_KEY!);
      spinner.succeed(`Total chunks in database: ${totalCount}`);

      console.log(chalk.green('\nâœ… Ingestion complete!\n'));
    } catch (error) {
      spinner.fail('Ingestion failed');
      console.error(chalk.red(`\nError: ${error instanceof Error ? error.message : String(error)}\n`));
      process.exit(1);
    }
  });

program
  .command('count')
  .description('Count chunks in database')
  .action(async () => {
    const { SUPABASE_URL, SUPABASE_SERVICE_KEY } = process.env;
    if (!SUPABASE_URL || !SUPABASE_SERVICE_KEY) {
      console.error(chalk.red('Error: SUPABASE_URL and SUPABASE_SERVICE_KEY must be set'));
      process.exit(1);
    }

    try {
      const count = await getChunkCount(SUPABASE_URL, SUPABASE_SERVICE_KEY);
      console.log(chalk.blue(`\nðŸ“Š Total chunks in database: ${count}\n`));
    } catch (error) {
      console.error(chalk.red(`Error: ${error instanceof Error ? error.message : String(error)}`));
      process.exit(1);
    }
  });

program.parse();
```

Key design decisions:
- Uses commander for CLI structure (standard)
- Uses ora for spinners with progress updates
- Uses chalk for colored output
- Validates env vars before running (fail fast)
- Dry-run mode shows sample chunks without API calls
- Progress callbacks update spinner text
- Count command for quick verification
- Default version is today's date (YYYY-MM-DD)
  </action>
  <verify>
Run `npx tsc --noEmit` (should compile without errors)
Run `npm run ingest -- --help` (should show help)
Run `npm run ingest -- run --dry-run --docs ./docs` (test with sample docs)
  </verify>
  <done>
CLI entry point orchestrates full pipeline with progress feedback, dry-run mode, and verification
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>Complete data ingestion pipeline with CLI interface</what-built>
  <how-to-verify>
1. **Setup Supabase** (if not done):
   - Go to Supabase Dashboard -> SQL Editor
   - Paste contents of `scripts/db/schema.sql`
   - Click "Run" to create table and function

2. **Create .env file** from .env.example:
   ```
   SUPABASE_URL=your_project_url
   SUPABASE_SERVICE_KEY=your_service_role_key
   OPENROUTER_API_KEY=your_openrouter_key
   ```

3. **Create test docs** (if no docs/ folder exists):
   ```bash
   mkdir -p docs
   echo "# Test Document\n\nThis is a test paragraph about Quilibrium.\n\n## Section One\n\nMore content here." > docs/test.md
   ```

4. **Run dry-run first**:
   ```bash
   npm run ingest -- run --dry-run
   ```
   Expected: Shows sample chunks with source file, heading path, token count

5. **Run full ingestion**:
   ```bash
   npm run ingest -- run
   ```
   Expected: Progress spinners for load -> chunk -> embed -> upload -> verify

6. **Verify in Supabase**:
   - Go to Supabase Dashboard -> Table Editor -> document_chunks
   - Should see rows with content, embedding (array), source_file, etc.

7. **Test similarity search** (in SQL Editor):
   ```sql
   -- Get a sample embedding
   SELECT embedding FROM document_chunks LIMIT 1;

   -- Use it to test the RPC function (paste the embedding array)
   SELECT * FROM match_document_chunks(
     '[0.1, 0.2, ...]'::vector(1536),  -- paste real embedding here
     0.5,  -- lower threshold for testing
     5
   );
   ```
   Expected: Returns rows with similarity scores
  </how-to-verify>
  <resume-signal>Type "approved" if pipeline works end-to-end, or describe any issues</resume-signal>
</task>

</tasks>

<verification>
After completing all tasks:
1. `npm run ingest -- --help` shows usage
2. `npm run ingest -- run --dry-run` previews without uploading
3. `npm run ingest -- run` completes full pipeline
4. `npm run ingest -- count` shows chunk count
5. Supabase table has chunks with embeddings
6. Similarity search RPC returns results
</verification>

<success_criteria>
- CLI script orchestrates load -> chunk -> embed -> upload
- Progress spinners show status during each phase
- Dry-run mode works without API keys
- Full run successfully uploads to Supabase
- Chunks have proper metadata (source_file, heading_path, version)
- Similarity search RPC function returns relevant results
- Re-running updates existing chunks (upsert works)
</success_criteria>

<output>
After completion, create `.planning/phases/01-data-pipeline/01-04-SUMMARY.md`
</output>
