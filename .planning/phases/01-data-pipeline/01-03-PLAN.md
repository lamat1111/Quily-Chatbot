---
phase: 01-data-pipeline
plan: 03
type: execute
wave: 2
depends_on: ["01-01"]
files_modified:
  - scripts/ingest/embedder.ts
  - scripts/ingest/uploader.ts
autonomous: true

must_haves:
  truths:
    - "Embedder generates 1536-dimensional vectors for text chunks"
    - "Embedder batches requests to avoid rate limits"
    - "Uploader stores chunks with embeddings in Supabase"
    - "Uploader uses upsert to prevent duplicates on re-ingestion"
  artifacts:
    - path: "scripts/ingest/embedder.ts"
      provides: "Batch embedding generation via OpenRouter"
      exports: ["generateEmbeddings"]
    - path: "scripts/ingest/uploader.ts"
      provides: "Supabase vector storage with upsert"
      exports: ["uploadChunks"]
  key_links:
    - from: "scripts/ingest/embedder.ts"
      to: "ai"
      via: "Vercel AI SDK embedMany"
      pattern: "embedMany.*from.*ai"
    - from: "scripts/ingest/embedder.ts"
      to: "@openrouter/ai-sdk-provider"
      via: "OpenRouter provider"
      pattern: "openrouter"
    - from: "scripts/ingest/uploader.ts"
      to: "@supabase/supabase-js"
      via: "Supabase client upsert"
      pattern: "supabase.*upsert"
---

<objective>
Implement embedding generation and Supabase storage for the ingestion pipeline.

Purpose: These modules transform text chunks into vectors and persist them in the database for similarity search.
Output: embedder.ts that generates embeddings via OpenRouter, uploader.ts that stores them in Supabase.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/01-data-pipeline/01-RESEARCH.md
@scripts/ingest/types.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create batch embedding generator</name>
  <files>scripts/ingest/embedder.ts</files>
  <action>
Create the embedder module that generates vectors via OpenRouter:

```typescript
import { embedMany } from 'ai';
import { createOpenRouter } from '@openrouter/ai-sdk-provider';
import type { ChunkWithContext, DocumentChunk } from './types.js';

// Batch size for embedding requests (stay under token limits)
const BATCH_SIZE = 50;

// Embedding model - text-embedding-3-small produces 1536 dimensions
const EMBEDDING_MODEL = 'openai/text-embedding-3-small';

/**
 * Format chunk content with heading context for better embeddings
 */
function formatForEmbedding(chunk: ChunkWithContext): string {
  if (chunk.metadata.heading_path) {
    return `${chunk.metadata.heading_path}\n\n${chunk.content}`;
  }
  return chunk.content;
}

/**
 * Generate embeddings for chunks in batches
 * @param chunks - Chunks with content and metadata
 * @param apiKey - OpenRouter API key
 * @param onProgress - Optional callback for progress updates
 * @returns Chunks with embeddings attached
 */
export async function generateEmbeddings(
  chunks: ChunkWithContext[],
  apiKey: string,
  onProgress?: (completed: number, total: number) => void
): Promise<DocumentChunk[]> {
  const openrouter = createOpenRouter({
    apiKey,
  });

  const results: DocumentChunk[] = [];
  const total = chunks.length;

  // Process in batches
  for (let i = 0; i < chunks.length; i += BATCH_SIZE) {
    const batch = chunks.slice(i, i + BATCH_SIZE);
    const texts = batch.map(formatForEmbedding);

    try {
      const { embeddings } = await embedMany({
        model: openrouter.textEmbeddingModel(EMBEDDING_MODEL),
        values: texts,
        maxRetries: 3,
      });

      // Attach embeddings to chunks
      for (let j = 0; j < batch.length; j++) {
        results.push({
          ...batch[j],
          embedding: embeddings[j],
        });
      }

      // Report progress
      if (onProgress) {
        onProgress(Math.min(i + BATCH_SIZE, total), total);
      }

      // Small delay between batches to avoid rate limits
      if (i + BATCH_SIZE < chunks.length) {
        await new Promise((resolve) => setTimeout(resolve, 100));
      }
    } catch (error) {
      // Re-throw with context about which batch failed
      const batchNum = Math.floor(i / BATCH_SIZE) + 1;
      const totalBatches = Math.ceil(chunks.length / BATCH_SIZE);
      throw new Error(
        `Embedding batch ${batchNum}/${totalBatches} failed: ${error instanceof Error ? error.message : String(error)}`
      );
    }
  }

  return results;
}
```

Key design decisions:
- Uses Vercel AI SDK embedMany for batch processing with retry logic
- Prepends heading context to chunk for better semantic embedding
- 50-item batches to stay well under OpenRouter limits
- Progress callback for CLI spinner updates
- Error messages include batch context for debugging
  </action>
  <verify>
Run `npx tsc --noEmit` (should compile without errors)
Check that file exports generateEmbeddings function
Check imports from 'ai' and '@openrouter/ai-sdk-provider'
  </verify>
  <done>
embedder.ts exports generateEmbeddings() that batches requests and attaches 1536-dim vectors to chunks
  </done>
</task>

<task type="auto">
  <name>Task 2: Create Supabase vector uploader</name>
  <files>scripts/ingest/uploader.ts</files>
  <action>
Create the uploader module that stores chunks in Supabase:

```typescript
import { createClient, SupabaseClient } from '@supabase/supabase-js';
import type { DocumentChunk } from './types.js';

// Batch size for database inserts
const UPLOAD_BATCH_SIZE = 100;

/**
 * Create Supabase client with service role key
 */
function getSupabaseClient(url: string, serviceKey: string): SupabaseClient {
  return createClient(url, serviceKey, {
    auth: {
      autoRefreshToken: false,
      persistSession: false,
    },
  });
}

/**
 * Upload document chunks with embeddings to Supabase
 * Uses upsert to handle re-ingestion without duplicates
 *
 * @param chunks - Document chunks with embeddings
 * @param supabaseUrl - Supabase project URL
 * @param supabaseKey - Supabase service role key
 * @param onProgress - Optional callback for progress updates
 */
export async function uploadChunks(
  chunks: DocumentChunk[],
  supabaseUrl: string,
  supabaseKey: string,
  onProgress?: (completed: number, total: number) => void
): Promise<{ inserted: number; errors: string[] }> {
  const supabase = getSupabaseClient(supabaseUrl, supabaseKey);
  const total = chunks.length;
  let inserted = 0;
  const errors: string[] = [];

  // Process in batches
  for (let i = 0; i < chunks.length; i += UPLOAD_BATCH_SIZE) {
    const batch = chunks.slice(i, i + UPLOAD_BATCH_SIZE);

    // Transform chunks to database format
    const rows = batch.map((chunk) => ({
      content: chunk.content,
      embedding: chunk.embedding,
      source_file: chunk.metadata.source_file,
      heading_path: chunk.metadata.heading_path || null,
      chunk_index: chunk.metadata.chunk_index,
      token_count: chunk.metadata.token_count,
      version: chunk.metadata.version,
      content_hash: chunk.metadata.content_hash,
    }));

    try {
      const { error } = await supabase
        .from('document_chunks')
        .upsert(rows, {
          onConflict: 'source_file,chunk_index',
          ignoreDuplicates: false, // Update existing rows
        });

      if (error) {
        errors.push(`Batch ${Math.floor(i / UPLOAD_BATCH_SIZE) + 1}: ${error.message}`);
      } else {
        inserted += batch.length;
      }
    } catch (err) {
      errors.push(
        `Batch ${Math.floor(i / UPLOAD_BATCH_SIZE) + 1}: ${err instanceof Error ? err.message : String(err)}`
      );
    }

    // Report progress
    if (onProgress) {
      onProgress(Math.min(i + UPLOAD_BATCH_SIZE, total), total);
    }
  }

  return { inserted, errors };
}

/**
 * Delete all chunks for a specific source file
 * Useful for re-ingesting a single document
 */
export async function deleteChunksForFile(
  sourceFile: string,
  supabaseUrl: string,
  supabaseKey: string
): Promise<{ deleted: number }> {
  const supabase = getSupabaseClient(supabaseUrl, supabaseKey);

  const { data, error } = await supabase
    .from('document_chunks')
    .delete()
    .eq('source_file', sourceFile)
    .select('id');

  if (error) {
    throw new Error(`Failed to delete chunks for ${sourceFile}: ${error.message}`);
  }

  return { deleted: data?.length ?? 0 };
}

/**
 * Get count of chunks in database
 * Useful for verification
 */
export async function getChunkCount(
  supabaseUrl: string,
  supabaseKey: string
): Promise<number> {
  const supabase = getSupabaseClient(supabaseUrl, supabaseKey);

  const { count, error } = await supabase
    .from('document_chunks')
    .select('*', { count: 'exact', head: true });

  if (error) {
    throw new Error(`Failed to count chunks: ${error.message}`);
  }

  return count ?? 0;
}
```

Key design decisions:
- Uses service role key (not anon) for insert permissions
- Upsert on source_file+chunk_index prevents duplicates
- Batched inserts (100 rows) for performance
- Helper functions for delete/count operations
- Progress callback for CLI spinner updates
- Returns both success count and error messages
  </action>
  <verify>
Run `npx tsc --noEmit` (should compile without errors)
Check that file exports uploadChunks function
Check imports from '@supabase/supabase-js'
  </verify>
  <done>
uploader.ts exports uploadChunks() that upserts vectors to Supabase, plus helper functions for delete and count
  </done>
</task>

</tasks>

<verification>
After completing all tasks:
1. `npx tsc --noEmit` succeeds
2. `ls scripts/ingest/embedder.ts scripts/ingest/uploader.ts` shows both files exist
3. embedder.ts uses EMBEDDING_MODEL constant (easy to change)
4. uploader.ts uses upsert with onConflict (prevents duplicates)
5. Both files have progress callbacks for CLI integration
</verification>

<success_criteria>
- embedder.ts generates 1536-dimensional embeddings via OpenRouter
- embedder.ts batches requests in groups of 50
- embedder.ts prepends heading context before embedding
- uploader.ts stores chunks with upsert (no duplicates)
- uploader.ts batches inserts in groups of 100
- Both provide progress callbacks
- TypeScript compiles without errors
</success_criteria>

<output>
After completion, create `.planning/phases/01-data-pipeline/01-03-SUMMARY.md`
</output>
